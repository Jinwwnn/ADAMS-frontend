[tool.poetry]
name = "RAG-LLM-Metric"
version = "0.1.0"
description = "My project description"
authors = ["Your Name <you@example.com>"]
packages = [
    {include = "evaluator", from = "backend"}, 
    {include = "data_annotator", from = "backend"}, 
    {include = "execution_pipeline", from = "backend"}, 
    {include = "agent", from = "backend"},
    {include = "utils", from = "backend"},
    {include = "api", from = "backend"}
]

[tool.poetry.dependencies]
python = "^3.11"
openai = "^1.12.0"
requests = "^2.31.0"
datasets = "^2.16.1"
python-dotenv = "^1.0.0"
ipykernel = "^6.19.0"
huggingface-hub = "^0.28"
transformers = "^4.48"
sentence-transformers = "^3.3"
# CPU version will be installed by default from PyPI
torch = { version = ">=2.1.2", optional = true }
ragas = "^0.2.14"
duckduckgo-search = "^7.5.5"
bert-score = "^0.3.13"
pandas = "^2.0.3"
numpy = "1.26.4"
streamlit = "^1.46.0"
plotly = "^6.1.2"
fastapi = "^0.115.14"
pydantic = "^2.11.7"
python-multipart = "^0.0.20"
uvicorn = {extras = ["standard"], version = "^0.24.0"}

[tool.poetry.group.dev.dependencies]
ipykernel = "^6.29.5"

# Add PyTorch CUDA repository as a source
[[tool.poetry.source]]
name = "pytorch"
url = "https://download.pytorch.org/whl/cu121"
priority = "explicit"

# Define optional GPU dependency (same package name but different source)
[tool.poetry.extras]
cpu = ["torch"]
gpu = ["torch"]

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[virtualenvs]
create = true
in-project = true